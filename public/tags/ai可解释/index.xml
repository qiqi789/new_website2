<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI可解释 on 齐琦 Qi Qi&#39;s Web portal</title>
    <link>//localhost:4321/tags/ai%E5%8F%AF%E8%A7%A3%E9%87%8A/</link>
    <description>Recent content in AI可解释 on 齐琦 Qi Qi&#39;s Web portal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Nov 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:4321/tags/ai%E5%8F%AF%E8%A7%A3%E9%87%8A/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>人工智能算法在可解释性和公平可信发展趋势的初步调研</title>
      <link>//localhost:4321/2022/11/08/test-new-post/</link>
      <pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate>
      <guid>//localhost:4321/2022/11/08/test-new-post/</guid>
      <description>人工智能算法在可解释性和公平可信发展趋势的初步调研1 现代人工智能的应用普及，同时也带来了算法在决策上的偏见和区别对待等问题。例如，一个犯罪风险评估算法对黑人比白人辩护人更有可能做出错误的预测判断；广告投放算法会把秘书和超市工作的广告更多地推介给女性；一个医疗资源配置算法对于同等需求的患者更倾向于白人。想比较于人类决策中的歧视现象，算法的歧视更具有隐蔽性，因为算法的预测模型是通过对大量收集的数据进行处理分析以后所确立的，而不是基于当事人的抉择。然而，传统的机器学习算法的模型构建和预测有可能强化了所应用的数据中的偏见与不均匀的配置，从而导致具有偏见的预测判断。&#xA;美国的现有法律明确禁止有目的歧视，比如差别对待。因此一个公司如果依赖于有歧视效应的预测模型来做出雇佣选择，那么它也是违反了相关法律[1]。然而，对于算法来说，如果仅是把一些诸如性别、种族等的特征数据从训练数据中刨除出去，也不一定能够保证算法的预测公平性。例如，在训练数据中，假如女性雇员的工作指标数据非常少，那么算法最终训练出来的模型可能对女性雇员的工作评价做出不公平的预测。因此，此类用于人力资源雇佣和评价决策的算法应该经常对其输入训练数据中不同群体的数据分布进行审计。&#xA;如何保证算法的公平性，计算机科学和数据科学专家已经提出了一些方法降低预测模型的可能偏见。这些方法通常会在构建模型过程中考虑应对敏感特征所代表的数据。然而，把不同群体的数据进行强制定量，忽视群体间内在的相关差异，也是不科学的。虽然，相关的法律规则并不容易被直接反应在计算机程序和算法里，但是，当前的法律为算法设计者、数据科学专家提供了比较多的自由，去探索不同的策略用于减少算法所产生的判别偏见[1]。&#xA;另一方面，人工智能持续为工业界提升全球经济总量，预计2030年将增加1300亿美元[2]。然而这种发展也受限于如何更有效地在各公司企业间分享数据。由于保密性和风险控制，大公司通常不愿意分享其数据。现代机器学习算法研究已经提供了解决这些问题的相关技术。比如，联邦转移学习技术，能够利用各公司分布式的数据训练构建跨公司的预测模型，同时解决数据分享的隐私保护，和各公司间数据分布不一致的问题[2]。人工智能模型的构建还需考虑其学习算法的可扩展性、鲁棒性，和算法表现的有效性。&#xA;此外，人工智能技术的迅速普及带动了数据驱动的模型构建的自动化进程，微软、谷歌、亚马逊都开发了所谓自动机器学习技术[3] ，通过给定数据能够自动产生预测模型，其预测表现不低于出版文献中提到的学习算法。&#xA;然而，在大数据时代，创建适合于人工智能的数据集依然面临很大的挑战。这些挑战包括数据的质量保证，和标签标注。据报道，数据科学家需要花费比模型训练、选择和布置上面多两倍的时间，用于数据的导入、清洗、可视化等数据准备工作上。这体现出在人工智能时代，数据收集、处理等准备工作拥有的巨大价值。&#xA;即使是现在最先进的人工智能模型也经常会受数据的影响产生错误的关联推断和偏见。对数据的处理和准备能够很大程度上影响使用这些数据进行训练的模型的泛化学习能力和可靠性。因此从数据为中心的角度看，人工智能的发展需要系统化和标准化的方法对训练和测试数据进行评估、合成、清洗和标注。这些方面都极大影响到了训练出的人工智能模型的预测判断表现。[3]&#xA;在文献[3]中，作者对人工智能数据管道中的每一个主要环节分别给出了一些关键建议。这些环节包括数据设计，数据处理和选择，数据评估策略等。整个数据管道也受数据有关法规政策的约束，以保证平衡、隐私保护的和可信的人工智能。在文章的最后，作者指出政府的法规和数据政策对于可信人工智能也扮演着重要的角色。例如，欧洲的AI规范草案中对构建模型的数据的规范性进行了明确的要求。有关的政策法规需要对隐私保护和数据访问之间进行合理的平衡。高质量和收集成本昂贵的数据集通常会成为公司私有的资产，学术界研究人员很难获取高质量的数据。政府在这方面的规范法规可以用来帮助改进数据的便携分享和控制。各国在这方面的努力仍处于初步阶段。同时，人工智能模型本身也可以用来对数据进行校准，检测其中的异常、错误和偏见。以数据为中心的方法将成为未来人工智能发展过程中不可缺少的组成部分。&#xA;[1] KIM P T. Addressing Algorithmic Discrimination[J/OL]. Commun. ACM, 2021, 65(1): 25-27. https://doi.org/10.1145/3498660.&#xA;[2] FINK O. NETLAND T. FEUERRIEGELC S. Artificial Intelligence across Company Borders[J/OL]. Commun. ACM, 2021, 65(1): 34-36. https://doi.org/10.1145/3470449.&#xA;[3] LIANG W. TADESSE G A. HO D. 等. Advances, challenges and opportunities in creating data for trustworthy AI[J/OL]. Nature Machine Intelligence, 2022, 4(8): 669-677. https://doi.org/10.1038/s42256-022-00516-1.&#xA;Email: qqi@hainanu.edu.cn&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
  </channel>
</rss>
